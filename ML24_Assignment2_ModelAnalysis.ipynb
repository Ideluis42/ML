{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2: Model Analysis\n",
        "\n",
        "As a reminder, here is the scenario from last time.\n",
        "\n",
        ">  You are the head of the R&D department at Pie-thagoras Labs, a new FoodTech start-up that is aimed at helping members of the visually impaired and blind community in different food-related situations. The first products you are making are image-based dessert classifiers that enable users to obtain detailed information about foods in front of them by simply taking a picture.\n",
        "\n",
        "In this assignment, you are going to be evaluating three different dessert classification models.  We are keeping the nature of the models and how most of the details about how they were trained a secret.  Instead, we want you to focus on making a determination about whether any of the three models are ready to be used by members of the blind community.  If so, what conditions would the models be suitable to be used in?\n",
        "\n",
        "\n",
        "### Training Data\n",
        "\n",
        "Each of the three models was trained on the same 18,400 images of desserts (800 images for each of 23 desserts). The models differ in how computationally expensive they are to run (model 1 is the fastest, model 2 is in the middle, and model 3 is the slowest), the details of how the training data was manipulated before being fed into the model, and the complexity of the model itself (model 1 is the simplest, model 2 is in the middle, and model 3 is the most complex).\n",
        "\n",
        "### Model Testing\n",
        "\n",
        "Along with each model, we have included the results of three test runs on each model (9 in total).  For testing, we ran each model with a set of 4600 images (200 images for each of the 23 dessert classifications that the models were trained on), yielding predictions for each inputted image.  The three test conditions were:\n",
        "\n",
        "* **Test Condition 1:** Images were resized so the smallest dimension (width or height was 256 pixels).  A 224x224 image was then cropped from the center of the image.\n",
        "* **Test Condition 2:** A random patch of the test image was cropped and then resized to 224x224 pixels.  Finally, with probability 1/2 the image was reflected across the vertical axis (mirroring the left and right side of the image).\n",
        "* **Test Condition 3:** Each image was randomly rotated by an angle between -180 and +180 degrees.  Next, each image were resized so the smallest dimension (with or height was 256 pixels).  Next, a 224x224 image was then cropped from the center of the image.  Finally, with probabilty 1/2 the image was reflected across the vertical axis (mirroring the left and right side of the image).\n",
        "\n",
        "These modifications might seem a bit confusing.  Never fear!  We're going to dive into how this all works next."
      ],
      "metadata": {
        "id": "hSW9WCz0mrWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll clone some the base repository for this assignment, and download the training data.  We'll also execute a command to show you where the data is downloaded."
      ],
      "metadata": {
        "id": "RBboTlIr77ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/olinml2024/machine_learning_a2\n",
        "%cd machine_learning_a2\n",
        "from data_utils import download_dessert_test_data\n",
        "download_dessert_test_data()\n",
        "!ls test_data/images"
      ],
      "metadata": {
        "id": "ID76XY_W7JBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `Files` functionality of Colab, you can browse to the location where the test data has been downloaded and open any of the images you'd like.  To access apple pie images, for example, you would navigate to `machine_learning_a2` -> `test_data` -> `images` -> `apple_pie` and then click on the image you want to view.\n",
        "\n",
        "We can also write some code to view sets of images.  There are quite a few ways to load and display these images, but we are going to make us of the `pytorch` library, since we will be using it a bunch later in the course.  Next, we'll show how to grab some images and display them."
      ],
      "metadata": {
        "id": "KYgDDfCd8i_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from model_testing.test import CLASS_INDICES_REVERSED\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def display_sample_images(image_transform):\n",
        "    test_set = datasets.ImageFolder('test_data/images', image_transform)\n",
        "    test_set_loader = DataLoader(test_set, batch_size=8, shuffle=True, num_workers=0)\n",
        "    inputs, outputs = next(iter(test_set_loader))\n",
        "    fig, axs = plt.subplots(nrows=2, ncols=4, squeeze=False)\n",
        "    for i in range(inputs.shape[0]):\n",
        "        img = inputs[i,:,:,:].squeeze().detach().numpy().transpose((1, 2, 0))\n",
        "        ax = axs[i // 4, i % 4]\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(f'{CLASS_INDICES_REVERSED[outputs[i].item()]}')\n",
        "        ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# This line of code specifies how we the data should be processed (this is\n",
        "# what was used for test scenario 1 as described earlier).  For details\n",
        "# of each of these functions you can examine the torchvision documentation.\n",
        "# https://pytorch.org/vision/stable/transforms.html\n",
        "test_scenario_1 = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor()])\n",
        "display_sample_images(test_scenario_1)"
      ],
      "metadata": {
        "id": "4kQeaoZO9CWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also specify the transforms used for test scenario 2.  Try running it a few times to get a sense of the variability."
      ],
      "metadata": {
        "id": "A32-tBFj-ukf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_scenario_2 = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor()])\n",
        "display_sample_images(test_scenario_2)"
      ],
      "metadata": {
        "id": "l8U1myF2-9FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can specify the transforms used for test scenario 3.  As before, try running it a few times to get a sense of the variability."
      ],
      "metadata": {
        "id": "cr_oG3iK_VJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_scenario_3 = transforms.Compose([transforms.RandomRotation(degrees=(-180, 180)),\n",
        "                                      transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor()])\n",
        "display_sample_images(test_scenario_3)"
      ],
      "metadata": {
        "id": "9n0N848J_bt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've seen the various was in which the images can be transformed, you might want to think about what conditions the end user of your system would encounter if they were to use this in the real world."
      ],
      "metadata": {
        "id": "m6F25VM5_8tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's load the predictions for each of our models.  Later in the notebook you can run the models yourself, but to make things faster, we've precomputed each model output for each of the three test scenarios.\n",
        "\n",
        "The code below loads a dictionary with the following test runs.  The table shows ground truth labels / predicted labels for each combination.\n",
        "\n",
        "| Model              | Test Condition 1 | Test Condition 2 | Test Condition 3 |\n",
        "| :---------------- | :------: | ----: | ----: |\n",
        "| Model 1        |   model1_test_1_gt / model1_test_1_preds  | model1_test_2_gt / model1_test_2_preds | model1_test_2_gt / model1_test_2_preds\n",
        "| Model 2        |   model2_test_1_gt / model2_test_1_preds  | model2_test_2_gt / model2_test_2_preds | model2_test_2_gt / model2_test_2_preds\n",
        "| Model 3        |   model3_test_1_gt / model3_test_1_preds  | model3_test_2_gt / model3_test_2_preds | model3_test_2_gt / model3_test_2_preds\n"
      ],
      "metadata": {
        "id": "sStZiS44ANrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('cached_outputs.json') as f:\n",
        "    model_outputs = json.load(f)\n",
        "\n",
        "model_outputs.keys()"
      ],
      "metadata": {
        "id": "Yt1yOUeAA3om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to these cached outputs, you can also run the models yourselves.  Next, we'll download all of the models and then run a model on an image from the web."
      ],
      "metadata": {
        "id": "OHcp3o5YB-QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "from model_utils import load_models, get_model_preds, test_on_image\n",
        "from PIL import Image\n",
        "import wget\n",
        "\n",
        "models = load_models()"
      ],
      "metadata": {
        "id": "-WQdv8inCKlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll run the models on data from the web and from an image in the repo.  You can also upload more images to test on these as  well."
      ],
      "metadata": {
        "id": "ejt10MCPCymO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_web_image(model, url):\n",
        "    file_name = wget.download(url)\n",
        "    img = Image.open(file_name)\n",
        "    output = test_on_image(model, img)\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False)\n",
        "    if output is not None:\n",
        "        plt.title(CLASS_INDICES_REVERSED[output])\n",
        "    plt.show()\n",
        "\n",
        "def test_on_local_image(model, image_path):\n",
        "    img = Image.open(image_path)\n",
        "    output = test_on_image(model, img)\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.tick_params(left = False, right = False , labelleft = False , labelbottom = False, bottom = False)\n",
        "    if output is not None:\n",
        "        plt.title(CLASS_INDICES_REVERSED[output])\n",
        "    plt.show()\n",
        "\n",
        "test_web_image(models[0], 'https://img.taste.com.au/FHudZ4_i/taste/2021/06/fairy-bread-churros-171749-2.jpg')\n",
        "test_web_image(models[2], 'https://assets.epicurious.com/photos/54b0226d766062b20344580a/1:1/w_2240,c_limit/51160200_glazed-doughnuts_1x1.jpg')\n",
        "\n",
        "test_on_local_image(models[1], 'images/cannoli-recipe-snippet.jpg')"
      ],
      "metadata": {
        "id": "dMPuRJdGuI5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, if you want (and have patience), you can rerun the models on the entire 4,600 test images.  If you do this, we recommend you swith Colab to a GPU runtime so it doesn't take too long.  **Most folks will not need to run the following code blocks, so we'll collapse them for now**"
      ],
      "metadata": {
        "id": "m0i5SWE3D_2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code for running all models on all testing scenarios\n",
        "\n",
        "test_condition_1 = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "test_condition_2 = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "test_condition_3 = transforms.Compose([\n",
        "        transforms.RandomRotation(degrees=(-180, 180)),\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "test_conditions = [test_condition_1, test_condition_2, test_condition_3]\n",
        "\n",
        "outputs = {}\n",
        "\n",
        "for idx, model in enumerate(models):\n",
        "    for idx2, condition in enumerate(test_conditions):\n",
        "        print(f\"Evaluating model {idx+1} test condition {idx2+1}\")\n",
        "        model_gt, model_preds = get_model_preds(model, condition)\n",
        "        outputs[f'model{idx+1}_test_{idx2+1}_gt'] = model_gt\n",
        "        outputs[f'model{idx+1}_test_{idx2+1}_preds'] = model_preds"
      ],
      "metadata": {
        "id": "IweDtmvmzw42",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code for downloading model outputs to the local machine (e.g, to add to Github)\n",
        "\n",
        "# download the cached data if useful\n",
        "from google.colab import files\n",
        "\n",
        "outputs_converted_for_json = {}\n",
        "for key, value in outputs.items():\n",
        "    value_changed = list(map(int, value))\n",
        "    outputs_converted_for_json[key] = value_changed\n",
        "\n",
        "with open('cached_outputs.json', 'w') as f:\n",
        "    json.dump(outputs_converted_for_json, f)\n",
        "\n",
        "files.download('cached_outputs.json')"
      ],
      "metadata": {
        "id": "mdN85rDoaytX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write-Up Requirements\n",
        "\n",
        "Now, you are ready to begin your analysis of each model. In your write-up, be sure to include each of the following:\n",
        "- Examine the test data.  What do you notice about it?  What conditions is it representative of (e.g., lighting, point of view, scale of the dessert relative to the rest of the image, etc.).\n",
        "- Utilize the functions from Assignment 1 to explore different estimates of accuracy for each model and test condition. Include analysis on how the metrics are relevant in the context of the problem of an app for dessert classification.\n",
        "- Evaluate the models on at least 10 images that are outside of the testing dataset (these could be images you take yourself and upload to Colab or they could be images from the web).  If you find an interesting test image, please feel free to share it with the class via Slack.\n",
        "- For each model you believe is fit for production, include an assessment of the conditions under which the model is likely to perform well and when it would be expected to perform poorly.  Be sure to explain your analysis (how did you arrive at this conclusion).  You will likely have to make some assumptions about the context in which the model would be used (e.g., by folks who are blind).  In the interests of time, you may make some assumptions (rather than doing a lot of background research).  Make sure to note your assumptions though (and what lingering questions you have).\n",
        "- For each model you believe is not fit for production, include an assessment of why you believe so.  Be sure to explain your analysis (how did you arrive at this conclusion).  In what ways should the models be improved to make them production ready?\n",
        "\n",
        "Be sure to attach any code/resources you used to create your write-up.\n",
        "\n",
        "\n",
        "Possible (optional) extensions:\n",
        "-  Transform the testing data in different was to better understand each model's performance.  You may consider changing the brightness of each image to evaluate the dependence on lighting conditions or changing the resolution of each image to determine if a certain level of quality is necessary for the algorithms to be successful.  In order to do this, you'll want to check out the [torchvision transforms module documentation](https://pytorch.org/vision/stable/transforms.html).\n",
        "- Benchmark the time it takes to run each model by modifying the given code that runs the model on the test set."
      ],
      "metadata": {
        "id": "VQ_lo0mD0jci"
      }
    }
  ]
}